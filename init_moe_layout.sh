#!/usr/bin/env bash
set -euo pipefail

# Usage: bash init_moe_layout.sh <MODEL_PATH>
MODEL_PATH_ARG="${1:-models/L3.1-MOE-13.7B/moe13b-q4ks.gguf}"

# project root = current dir
ROOT="$(pwd)"
MOE_DIR="$ROOT/moe"
DOCS_DIR="$MOE_DIR/docs"

mkdir -p "$MOE_DIR" "$DOCS_DIR"

# 1) environment.yml (conda)
cat > "$MOE_DIR/environment.yml" <<'YML'
name: llmserve
channels: [defaults, conda-forge, nvidia, pytorch]
dependencies:
  - python=3.10
  - pip
  - pip:
      - "llama-cpp-python[cuda]"
      - fastapi
      - uvicorn
      - gradio
      - pydantic
      - python-dotenv
      - huggingface_hub
      - hf-transfer
YML

# 2) .env (runtime config)
cat > "$MOE_DIR/.env" <<ENV
# ---- Runtime config (edit as needed) ----
MODEL_PATH=$MODEL_PATH_ARG
HOST=0.0.0.0
PORT=7860
N_CTX=4096
N_GPU_LAYERS=-1
N_BATCH=512
CONCURRENCY=2
TEMPERATURE=0.8
TOP_P=0.9
# Hugging Face cache (optional)
HF_HOME=\$HOME/.cache/huggingface
HF_HUB_ENABLE_HF_TRANSFER=1
ENV

# 3) server.py (å ä½æ¨¡æ¿ï¼šåç»­å†å¡«å®ç°)
cat > "$MOE_DIR/server.py" <<'PY'
"""
å ä½æ–‡ä»¶ï¼šæœåŠ¡å…¥å£ï¼ˆGradio + FastAPIï¼‰ã€‚
ä½ åé¢è¯´â€œå¯ä»¥å†™ä»£ç äº†â€æ—¶ï¼Œå†æŠŠå®ç°ç²˜è¿‡æ¥ã€‚
"""
if __name__ == "__main__":
    print("server.py placeholder. Fill implementation later.")
PY

# 4) systemd å•å…ƒï¼ˆæœ¬åœ°æ¨¡æ¿ï¼‰
cat > "$MOE_DIR/llm-serve.service" <<'UNIT'
# æ‹¿åˆ°æœåŠ¡å™¨åæ”¾åˆ° /etc/systemd/system/ å†å¯ç”¨
[Unit]
Description=LLM Inference Server
After=network.target

[Service]
Type=simple
User=%i
WorkingDirectory=/srv/llm
EnvironmentFile=/srv/llm/.env
ExecStart=/bin/bash -lc 'conda activate llmserve && uvicorn server:app --host ${HOST} --port ${PORT}'
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
UNIT

# 5) Nginx åä»£ï¼ˆå¯é€‰ï¼‰
cat > "$MOE_DIR/nginx.conf" <<'NG'
# æ”¾åˆ° /etc/nginx/conf.d/ å¹¶æ”¹åŸŸå/è¯ä¹¦å reload
server {
  listen 80;
  server_name _;

  location / {
    proxy_pass http://127.0.0.1:7860;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
    proxy_read_timeout 3600s;
    proxy_send_timeout 3600s;
  }
}
NG

# 6) è¿è¡Œ/éªŒè¯è„šæœ¬
cat > "$MOE_DIR/start.sh" <<'SH'
#!/usr/bin/env bash
set -euo pipefail
source ~/.bashrc || true
conda activate llmserve
export $(grep -v '^#' .env | xargs -d '\n') || true
python -c "import sys; print('Using .env MODEL_PATH=', '${MODEL_PATH}')" || true
uvicorn server:app --host "${HOST:-0.0.0.0}" --port "${PORT:-7860}"
SH
chmod +x "$MOE_DIR/start.sh"

cat > "$MOE_DIR/healthcheck.sh" <<'SH'
#!/usr/bin/env bash
set -euo pipefail
HOST="${1:-127.0.0.1}"
PORT="${2:-7860}"
set -x
curl -s "http://${HOST}:${PORT}/healthz" || true
SH
chmod +x "$MOE_DIR/healthcheck.sh"

# 7) æ–‡æ¡£æ¨¡æ¿
cat > "$DOCS_DIR/MODEL_MANIFEST.md" <<MAN
# MODEL_MANIFEST

- Model file: \`$MODEL_PATH_ARG\`
- Format: GGUF
- Quantization: q4_k_s (ç¤ºä¾‹)
- Source repo: (å¡«å†™)
- Commit / Date: (å¡«å†™)
- Target GPU / VRAM: (å¡«å†™)
- Notes: (å¦‚éœ€ tokenizer å…¼å®¹æ€§/rope è®¾ç½®ç­‰)
MAN

cat > "$DOCS_DIR/SMOKE.md" <<'MD'
# SMOKE / éªŒæ”¶æ­¥éª¤ï¼ˆç¤ºä¾‹ï¼‰
1. conda env: `conda env create -f environment.yml && conda activate llmserve`
2. å¡«å¥½ `.env` çš„ `MODEL_PATH`
3. å¯åŠ¨ï¼š`bash start.sh`
4. å¥åº·æ£€æŸ¥ï¼š`bash healthcheck.sh <ip> 7860`
5. REST è°ƒç”¨ï¼ˆç¤ºä¾‹ï¼‰ï¼š
curl -s -X POST http://<ip>:7860/v1/completions
-H "Content-Type: application/json"
-d '{"prompt":"hello","max_tokens":16,"stream":false}'
6. æµè§ˆå™¨æ‰“å¼€ `http://<ip>:7860/` çœ‹ Gradioï¼ˆå®ç°åå¯ç”¨ï¼‰
MD

cat > "$DOCS_DIR/LOGGING.md" <<'MD'
# LOGGING / æ’é”™ç¬”è®°ï¼ˆç¤ºä¾‹ï¼‰
- è¿è¡Œæ—¥å¿—ï¼š`journalctl -u llm-serve -f`ï¼ˆä¸Šåˆ°æœåŠ¡å™¨åï¼‰
- Nginx æ—¥å¿—ï¼š`/var/log/nginx/access.log`, `/var/log/nginx/error.log`
- å¸¸è§é—®é¢˜ï¼š
- OOMï¼šè°ƒå° N_BATCH / N_CTX / å¹¶å‘ï¼›æˆ–æ¢æ›´å°é‡åŒ–
- é¦– token æ…¢ï¼šç¡®è®¤å·²é¢„çƒ­ã€æ¨¡å‹åœ¨ NVMeã€è¿›ç¨‹å¸¸é©»
MD

echo "âœ… Created templates under: $MOE_DIR"
echo "ğŸ‘‰ Next:"
echo "   1) æ‰“å¼€ $MOE_DIR/.env æŠŠ MODEL_PATH æ”¹æˆä½ çš„å®é™… GGUF è·¯å¾„"
echo "   2) conda env:  conda env create -f $MOE_DIR/environment.yml && conda activate llmserve"
echo "   3) ç­‰ä½ è¯´â€œå¯ä»¥å†™ä»£ç äº†â€ï¼Œæˆ‘å†æŠŠ server.py çš„å®ç°å¡«ä¸Š"