{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509179e5",
   "metadata": {},
   "source": [
    "# Dual Model Chat Interface\n",
    "This notebook provides interfaces to both:\n",
    "1. **DeepSeek-V2-Lite-Chat** (PyTorch/Transformers) - Full precision model\n",
    "2. **L3.1-MOE-13.7B** (GGUF/llama.cpp) - Quantized model via API\n",
    "\n",
    "Use this for comparing model responses and testing different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578c2d4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8517df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4c766",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alpaca sample data\n",
    "def load_sample_data():\n",
    "    with open('/home/lmx/EchoPersona/alpaca_sample.jsonl', 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "# Get a few sample prompts\n",
    "sample_data = load_sample_data()\n",
    "print(f\"Loaded {len(sample_data)} samples\")\n",
    "\n",
    "# Display first few samples\n",
    "for i, sample in enumerate(sample_data[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {sample['instruction']}\")\n",
    "    if sample['input']:\n",
    "        print(f\"Input: {sample['input']}\")\n",
    "    print(f\"Expected Output: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65ab57",
   "metadata": {},
   "source": [
    "## Model 1: DeepSeek-V2-Lite-Chat (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DeepSeek model\n",
    "model_dir = \"/home/lmx/EchoPersona/models/DeepSeek-V2-Lite-Chat\"\n",
    "\n",
    "print(\"Loading DeepSeek tokenizer and model...\")\n",
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DeepSeek model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fb2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_deepseek(instruction, input_text=\"\", max_tokens=512, temperature=0.8):\n",
    "    \"\"\"Chat with DeepSeek model\"\"\"\n",
    "    # Format chat\n",
    "    if input_text:\n",
    "        prompt = f\"<|user|>{instruction}\\n\\nInput: {input_text}<|assistant|>\"\n",
    "    else:\n",
    "        prompt = f\"<|user|>{instruction}<|assistant|>\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = deepseek_tokenizer(prompt, return_tensors=\"pt\").to(deepseek_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = deepseek_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=deepseek_tokenizer.eos_token_id,\n",
    "            pad_token_id=deepseek_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_response = deepseek_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test with a sample\n",
    "test_instruction = \"Give three tips for staying healthy.\"\n",
    "deepseek_response = chat_with_deepseek(test_instruction)\n",
    "print(f\"DeepSeek Response:\\n{deepseek_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a4bcf",
   "metadata": {},
   "source": [
    "## Model 2: L3.1-MOE-13.7B (via API Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e8eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_moe_api(instruction, input_text=\"\", max_tokens=256, temperature=0.8, api_url=\"http://localhost:7860\"):\n",
    "    \"\"\"Chat with MOE model via API\"\"\"\n",
    "    # Format prompt\n",
    "    if input_text:\n",
    "        prompt = f\"Instruction: {instruction}\\n\\nInput: {input_text}\\n\\nResponse:\"\n",
    "    else:\n",
    "        prompt = f\"Instruction: {instruction}\\n\\nResponse:\"\n",
    "    \n",
    "    # API request\n",
    "    payload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{api_url}/v1/completions\", json=payload, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"text\"].strip()\n",
    "        else:\n",
    "            return f\"API Error: {response.status_code} - {response.text}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Connection Error: {e}\\n\\nMake sure the MOE server is running:\\npython /home/lmx/EchoPersona/moe/server.py\"\n",
    "\n",
    "# Test API connection\n",
    "def check_moe_server():\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:7860/healthz\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            info = response.json()\n",
    "            print(f\"‚úÖ MOE Server is running\")\n",
    "            print(f\"Model: {info['model_path']}\")\n",
    "            print(f\"Context: {info['n_ctx']}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Server responded with {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException:\n",
    "        print(\"‚ùå MOE Server not responding\")\n",
    "        print(\"Start it with: python /home/lmx/EchoPersona/moe/server.py\")\n",
    "        return False\n",
    "\n",
    "server_running = check_moe_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092449fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MOE model if server is running\n",
    "if server_running:\n",
    "    moe_response = chat_with_moe_api(test_instruction)\n",
    "    print(f\"MOE Response:\\n{moe_response}\")\n",
    "else:\n",
    "    print(\"Skipping MOE test - server not running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fa450",
   "metadata": {},
   "source": [
    "## Compare Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8482c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(instruction, input_text=\"\", max_tokens=256):\n",
    "    \"\"\"Compare responses from both models\"\"\"\n",
    "    print(f\"üî• Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"üìù Input: {input_text}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # DeepSeek response\n",
    "    print(\"\\nü§ñ DeepSeek-V2-Lite-Chat (PyTorch):\")\n",
    "    print(\"-\" * 40)\n",
    "    deepseek_resp = chat_with_deepseek(instruction, input_text, max_tokens)\n",
    "    print(deepseek_resp)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # MOE response\n",
    "    print(\"\\nüöÄ L3.1-MOE-13.7B (GGUF):\")\n",
    "    print(\"-\" * 40)\n",
    "    if server_running:\n",
    "        moe_resp = chat_with_moe_api(instruction, input_text, max_tokens)\n",
    "        print(moe_resp)\n",
    "    else:\n",
    "        print(\"‚ùå Server not running. Start with: python /home/lmx/EchoPersona/moe/server.py\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test comparison\n",
    "compare_models(\"Explain the concept of machine learning in simple terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca9b73",
   "metadata": {},
   "source": [
    "## Batch Testing with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple samples\n",
    "def batch_test(num_samples=3):\n",
    "    \"\"\"Test both models on multiple samples\"\"\"\n",
    "    for i in range(min(num_samples, len(sample_data))):\n",
    "        sample = sample_data[i]\n",
    "        print(f\"\\n{'='*20} SAMPLE {i+1} {'='*20}\")\n",
    "        compare_models(sample['instruction'], sample['input'], max_tokens=200)\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run batch test\n",
    "batch_test(2)  # Test first 2 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36a5e4",
   "metadata": {},
   "source": [
    "## Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf63ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive chat function\n",
    "def interactive_chat():\n",
    "    print(\"ü§ñ Interactive Dual Model Chat\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        compare_models(user_input, max_tokens=300)\n",
    "\n",
    "# Uncomment to start interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c19c9e",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa946932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "def cleanup_memory():\n",
    "    import gc\n",
    "    global deepseek_model, deepseek_tokenizer\n",
    "    \n",
    "    try:\n",
    "        del deepseek_model\n",
    "        del deepseek_tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ Memory cleaned up\")\n",
    "\n",
    "# Uncomment to clean up memory\n",
    "# cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
