{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509179e5",
   "metadata": {},
   "source": [
    "# Dual Model Chat Interface\n",
    "This notebook provides interfaces to both:\n",
    "1. **DeepSeek-V2-Lite-Chat** (PyTorch/Transformers) - Full precision model\n",
    "2. **L3.1-MOE-13.7B** (GGUF/llama.cpp) - Quantized model via API\n",
    "\n",
    "Use this for comparing model responses and testing different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578c2d4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8517df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5080\n",
      "Memory: 15.9 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4c766",
   "metadata": {},
   "source": [
    "## Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7f6b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 samples\n",
      "\n",
      "--- Sample 1 ---\n",
      "Instruction: Give three tips for staying healthy.\n",
      "Expected Output: 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and...\n",
      "\n",
      "--- Sample 2 ---\n",
      "Instruction: What are the three primary colors?\n",
      "Expected Output: The three primary colors are red, blue, and yellow. These colors are called primary because they can...\n",
      "\n",
      "--- Sample 3 ---\n",
      "Instruction: Describe the structure of an atom.\n",
      "Expected Output: An atom is the basic building block of all matter and is made up of three types of particles: proton...\n"
     ]
    }
   ],
   "source": [
    "def load_sample_data():\n",
    "    with open('/home/lmx/EchoPersona/alpaca_sample.jsonl', 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "sample_data = load_sample_data()\n",
    "print(f\"Loaded {len(sample_data)} samples\")\n",
    "\n",
    "for i, sample in enumerate(sample_data[:3]):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Instruction: {sample['instruction']}\")\n",
    "    if sample['input']:\n",
    "        print(f\"Input: {sample['input']}\")\n",
    "    print(f\"Expected Output: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65ab57",
   "metadata": {},
   "source": [
    "## Model 1: DeepSeek-V2-Lite-Chat (Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cda65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeepSeek tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmx/miniconda3/envs/dl/lib/python3.10/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 671089664 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898288737ff44a9d94c31d79899562b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load DeepSeek model\n",
    "model_dir = \"/home/lmx/EchoPersona/models/DeepSeek-V2-Lite-Chat\"\n",
    "\n",
    "print(\"Loading DeepSeek tokenizer and model...\")\n",
    "deepseek_tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "deepseek_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"DeepSeek model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fb2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek Response:\n",
      "Love can be complex and subjective, as it means different things to different people. However, here's a general definition:\n",
      "* Love is a deep, emotional connection or attachment towards someone or something that inspires fond feelings and care.* Love often involves strong affection, compassion, and kindness towards another person or entity.* It can also involve the desire for an individual to protect, support, and feel connected with the one who is loved.* In many cultures around the world, love plays a central role in relationships between partners, family members, friends, and even communities.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_deepseek(instruction, input_text=\"\", max_tokens=512, temperature=0.8):\n",
    "    \"\"\"Chat with DeepSeek model\"\"\"\n",
    "    if input_text:\n",
    "        prompt = f\"<|user|>{instruction}\\n\\nInput: {input_text}<|assistant|>\"\n",
    "    else:\n",
    "        prompt = f\"<|user|>{instruction}<|assistant|>\"\n",
    "    \n",
    "    inputs = deepseek_tokenizer(prompt, return_tensors=\"pt\").to(deepseek_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = deepseek_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=deepseek_tokenizer.eos_token_id,\n",
    "            pad_token_id=deepseek_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_response = deepseek_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(prompt):].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "test_instruction = \"Explain quantum computing in simple terms\"\n",
    "deepseek_response = chat_with_deepseek(test_instruction)\n",
    "print(f\"DeepSeek Response:\\n{deepseek_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a4bcf",
   "metadata": {},
   "source": [
    "## Model 2: L3.1-MOE-13.7B (via API Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e8eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå MOE Server not responding\n",
      "Start it with: python /home/lmx/EchoPersona/moe/server.py\n"
     ]
    }
   ],
   "source": [
    "def chat_with_moe_direct(instruction, input_text=\"\", max_tokens=256, temperature=0.8):\n",
    "    \"\"\"Chat with MOE model directly using llama-cpp-python\"\"\"\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        import os\n",
    "        \n",
    "        model_path = \"/home/lmx/EchoPersona/models/L3.1-MOE-13.7B/moe13b-q4ks.gguf\"\n",
    "        \n",
    "        if not hasattr(chat_with_moe_direct, 'llm'):\n",
    "            print(\"Loading MOE model directly...\")\n",
    "            chat_with_moe_direct.llm = Llama(\n",
    "                model_path=model_path,\n",
    "                n_ctx=4096,\n",
    "                n_gpu_layers=-1,\n",
    "                n_batch=512,\n",
    "                verbose=False,\n",
    "            )\n",
    "            print(\"MOE model loaded successfully\")\n",
    "        \n",
    "        if input_text:\n",
    "            prompt = f\"Instruction: {instruction}\\n\\nInput: {input_text}\\n\\nResponse:\"\n",
    "        else:\n",
    "            prompt = f\"Instruction: {instruction}\\n\\nResponse:\"\n",
    "        \n",
    "        response = chat_with_moe_direct.llm.create_completion(\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            stream=False,\n",
    "        )\n",
    "        \n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error loading MOE model: {e}\"\n",
    "\n",
    "def check_moe_server():\n",
    "    \"\"\"For compatibility - always return False since we're using direct access\"\"\"\n",
    "    print(\"Using direct MOE model access (bypassing server)\")\n",
    "    return False\n",
    "\n",
    "server_running = check_moe_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092449fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:  # Always test MOE model since we're using direct access\n",
    "    moe_response = chat_with_moe_direct(test_instruction)\n",
    "    print(f\"MOE Response:\\n{moe_response}\")\n",
    "else:\n",
    "    print(\"Skipping MOE test - using direct access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fa450",
   "metadata": {},
   "source": [
    "## Compare Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8482c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(instruction, input_text=\"\", max_tokens=256):\n",
    "    \"\"\"Compare responses from both models\"\"\"\n",
    "    print(f\"üî• Query: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"üìù Input: {input_text}\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    print(\"\\nü§ñ DeepSeek-V2-Lite-Chat (PyTorch):\")\n",
    "    print(\"-\" * 40)\n",
    "    deepseek_resp = chat_with_deepseek(instruction, input_text, max_tokens)\n",
    "    print(deepseek_resp)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    print(\"\\nüöÄ L3.1-MOE-13.7B (GGUF):\")\n",
    "    print(\"-\" * 40)\n",
    "    moe_resp = chat_with_moe_direct(instruction, input_text, max_tokens)\n",
    "    print(moe_resp)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "compare_models(\"What are the key differences between Python and JavaScript?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca9b73",
   "metadata": {},
   "source": [
    "## Batch Testing with Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple samples\n",
    "def batch_test(num_samples=3):\n",
    "    \"\"\"Test both models on multiple samples\"\"\"\n",
    "    for i in range(min(num_samples, len(sample_data))):\n",
    "        sample = sample_data[i]\n",
    "        print(f\"\\n{'='*20} SAMPLE {i+1} {'='*20}\")\n",
    "        compare_models(sample['instruction'], sample['input'], max_tokens=200)\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run batch test\n",
    "batch_test(2)  # Test first 2 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a36a5e4",
   "metadata": {},
   "source": [
    "## Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf63ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    print(\"ü§ñ Dual Model Chat - Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \")\n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "        \n",
    "        compare_models(user_input, max_tokens=300)\n",
    "\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c19c9e",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa946932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "def cleanup_memory():\n",
    "    import gc\n",
    "    global deepseek_model, deepseek_tokenizer\n",
    "    \n",
    "    try:\n",
    "        del deepseek_model\n",
    "        del deepseek_tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ Memory cleaned up\")\n",
    "\n",
    "# Uncomment to clean up memory\n",
    "# cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
